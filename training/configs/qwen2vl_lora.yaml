# Qwen2-VL LoRA/QLoRA Fine-tuning Configuration for CircuitSense
# Optimized for NVIDIA A6000 (48GB VRAM)
# Usage: PYTHONPATH=. python training/train.py --config training/configs/qwen2vl_lora.yaml
#
# A6000 Optimizations (Qwen2-VL specific):
# - Batch size: 2 (vision-language models need smaller batches due to vision encoder)
# - LoRA rank: 64 (reduced to save memory, can increase to 128 if memory allows)
# - Gradient accumulation: 8 (effective batch size = 2 * 8 = 16)
# - Max seq length: 2048 (reduced from 4096 to save memory)
# - 8-bit optimizer: Saves ~2GB memory
# - Gradient checkpointing: Enabled for memory efficiency
#
# Memory Usage Estimate (Qwen2-VL):
# - Base model (bf16): ~14GB
# - Vision encoder: ~4-6GB
# - LoRA weights (r=64): ~0.3GB
# - Activations (batch=2, seq=2048): ~15-20GB
# - Optimizer states: ~1-2GB
# - Total: ~35-42GB (safe margin on 48GB GPU)
#
# If still OOM, try:
# 1. Enable QLoRA (quantization.enabled: true) - saves ~50% memory
# 2. Reduce batch_size to 1
# 3. Reduce max_seq_length to 1024
# 4. Reduce LoRA rank to 32

# Model configuration
model:
  name: "Qwen/Qwen2-VL-7B-Instruct"  # or "Qwen/Qwen2-VL-72B-Instruct" for larger model
  torch_dtype: "bfloat16"             # bfloat16 recommended for training
  use_flash_attention: true          # Disable Flash Attention 2 (requires flash_attn package)
  trust_remote_code: true             # Required for Qwen2-VL

# Quantization configuration (for QLoRA)
quantization:
  enabled: false                       # Set to true to enable QLoRA (saves ~50% memory)
                                        # Keep false for standard LoRA (better quality)
  bits: 4                             # 4-bit quantization
  quant_type: "nf4"                   # NormalFloat4 quantization
  double_quant: true                  # Use double quantization
  compute_dtype: "bfloat16"           # Compute dtype for quantized layers

# LoRA configuration
lora:
  r: 64                               # LoRA rank (reduced from 128 to save memory)
  alpha: 128                          # LoRA alpha (typically 2x rank for good scaling)
  dropout: 0.05                       # Small dropout for regularization
  target_modules:                     # Modules to apply LoRA
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"                        # Don't train bias
  task_type: "CAUSAL_LM"              # Task type for PEFT

# Training configuration (optimized for A6000 48GB VRAM)
training:
  output_dir: "outputs/qwen2vl-circuitsense"
  num_epochs: 10
  per_device_train_batch_size: 2      # Reduced for Qwen2-VL (images + vision encoder = high memory)
  per_device_eval_batch_size: 2       # Match train batch size
  gradient_accumulation_steps: 8      # Effective batch size = 2 * 8 = 16
  learning_rate: 2.0e-4               # Learning rate for LoRA (good default)
  weight_decay: 0.01
  warmup_ratio: 0.03                  # Warmup steps as ratio of total
  lr_scheduler_type: "cosine"         # Learning rate schedule
  max_seq_length: 2048                # Reduced from 4096 to save memory (can increase if needed)
  gradient_checkpointing: true        # Essential for memory efficiency
  optim: "adamw_8bit"                 # 8-bit optimizer saves ~2GB memory
  fp16: false                         # Use bf16 instead
  bf16: true                          # BFloat16 training (better than fp16)
  max_grad_norm: 1.0                  # Gradient clipping
  seed: 42                            # Random seed

# Data configuration
data:
  data_dir: "datasets/training_data"  # Path to dataset directory
  dataset_type: "auto"                # "auto", "qa_folder", "generated", or "huggingface"
                                      # "auto" detects QA folder structure (q1/, q2/, etc.) automatically
  task_type: "both"                   # "equations", "qa", or "both" (for generated dataset type)
  train_split: 0.9                    # Train/val split ratio
  augment_questions: true             # Use question template augmentation (for generated dataset type)

# Logging and checkpointing
logging:
  report_to: "wandb"                  # "wandb", "tensorboard", or "none"
  project_name: "circuitsense-finetune"
  run_name: null                      # Auto-generated if null
  logging_steps: 10                   # Log every N steps
  save_strategy: "steps"              # "steps" or "epoch"
  save_steps: 500                     # Save checkpoint every N steps
  save_total_limit: 3                 # Keep only N recent checkpoints
  eval_strategy: "steps"              # "steps" or "epoch"
  eval_steps: 500                     # Evaluate every N steps
  load_best_model_at_end: true        # Load best checkpoint at end
  metric_for_best_model: "eval_loss"  # Metric to determine best model

# DeepSpeed configuration (for multi-GPU training)
deepspeed:
  enabled: false                      # Set to true for multi-GPU
  config_file: null                   # Path to deepspeed config, or use preset
  stage: 2                            # ZeRO stage (2 or 3)

# Evaluation configuration
evaluation:
  do_eval: true                       # Run evaluation during training
  eval_on_start: false                # Evaluate before training
  prediction_loss_only: false         # Also compute other metrics
