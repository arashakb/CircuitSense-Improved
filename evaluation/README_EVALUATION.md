# Qwen2-VL Evaluation Guide

This guide explains how to evaluate Qwen2-VL models (base or fine-tuned) on the CircuitSense symbolic equations benchmark.

## Overview

The evaluation script (`evaluate_qwen2vl.py`) allows you to:
- Test the base Qwen2-VL-7B-Instruct model before fine-tuning
- Test fine-tuned models (LoRA adapters or merged models) after training
- Compare model performance before and after fine-tuning
- Evaluate model accuracy on symbolic equation derivation tasks

The script uses the same equation comparison logic as `benchmark_symbolic_equations.py` but loads the model locally instead of using APIs.

## Dataset Format

The script expects the symbolic equations dataset in the following structure:

```
datasets/symbolic_level15_27/
├── q1/
│   ├── q1_question.txt      # Question text
│   ├── q1_image.png         # Circuit diagram
│   ├── q1_ta.txt            # Ground truth answer (symbolic equation)
│   └── q1_qwen2vl.txt       # Model response (generated by script)
├── q2/
│   ├── q2_question.txt
│   ├── q2_image.png
│   ├── q2_ta.txt
│   └── q2_qwen2vl.txt
└── ...
```

## Installation

Ensure you have all required dependencies:

```bash
pip install torch transformers peft accelerate sympy pandas pillow
```

For flash attention support (recommended):
```bash
pip install flash-attn
```

## Usage

### Basic Usage

#### 1. Evaluate Base Model (Before Fine-tuning)

```bash
PYTHONPATH=. python evaluation/evaluate_qwen2vl.py \
    --model Qwen/Qwen2-VL-7B-Instruct \
    --dataset_path datasets/symbolic_level15_27 \
    --mode full \
    --max_questions 10
```

This will:
- Load the base model from HuggingFace
- Run inference on 10 questions
- Evaluate the responses against ground truth
- Save results to `results/results_qwen_qwen2-vl-7b-instruct_TIMESTAMP.csv`

#### 2. Evaluate Fine-tuned Model (After Training)

```bash
PYTHONPATH=. python evaluation/evaluate_qwen2vl.py \
    --model outputs/circuitsense_finetune/final_model \
    --dataset_path datasets/symbolic_level15_27 \
    --mode full \
    --max_questions 50
```

This works with:
- **LoRA adapters**: Automatically detects and loads the base model + adapter
- **Merged models**: Loads the full merged model

### Modes

The script supports three modes:

#### Mode 1: Inference Only (`--mode inference`)

Generates responses and saves them to files without evaluating:

```bash
PYTHONPATH=. python evaluation/evaluate_qwen2vl.py \
    --model Qwen/Qwen2-VL-7B-Instruct \
    --dataset_path datasets/symbolic_level15_27 \
    --mode inference \
    --max_questions 100
```

**Use case**: Generate all responses first, then evaluate later (useful for large datasets).

**Output**: 
- Saved responses in each question folder (e.g., `q1/q1_modelname.txt`)
- CSV file with inference statistics

#### Mode 2: Evaluation Only (`--mode evaluation`)

Evaluates existing saved responses:

```bash
PYTHONPATH=. python evaluation/evaluate_qwen2vl.py \
    --model outputs/circuitsense_finetune/final_model \
    --dataset_path datasets/symbolic_level15_27 \
    --mode evaluation \
    --max_questions 100
```

**Use case**: Re-evaluate responses from a previous run, or evaluate responses generated by a different model.

**Requirements**: Responses must already exist in the question folders.

#### Mode 3: Full Benchmark (`--mode full`, default)

Runs inference and evaluation together:

```bash
PYTHONPATH=. python evaluation/evaluate_qwen2vl.py \
    --model outputs/circuitsense_finetune/final_model \
    --dataset_path datasets/symbolic_level15_27 \
    --mode full \
    --max_questions 50
```

**Use case**: Standard evaluation workflow.

### Command-Line Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--model` | str | **required** | Path to model (HF model ID or local path) |
| `--dataset_path` | str | **required** | Path to symbolic equations dataset |
| `--mode` | str | `full` | Mode: `inference`, `evaluation`, or `full` |
| `--max_questions` | int | `600` | Maximum number of questions to process |
| `--device` | str | `cuda` | Device to use (`cuda` or `cpu`) |
| `--torch_dtype` | str | `bfloat16` | Torch dtype (`float32`, `float16`, `bfloat16`) |
| `--no_flash_attention` | flag | False | Disable flash attention |
| `--temperature` | float | `0.1` | Sampling temperature (0.0 = deterministic) |
| `--max_new_tokens` | int | `512` | Maximum tokens to generate |

## Workflow: Before and After Fine-tuning

### Step 1: Evaluate Base Model

```bash
# Generate responses with base model
PYTHONPATH=. python evaluation/evaluate_qwen2vl.py \
    --model Qwen/Qwen2-VL-7B-Instruct \
    --dataset_path datasets/symbolic_level15_27 \
    --mode inference \
    --max_questions 100

# Evaluate base model responses
PYTHONPATH=. python evaluation/evaluate_qwen2vl.py \
    --model Qwen/Qwen2-VL-7B-Instruct \
    --dataset_path datasets/symbolic_level15_27 \
    --mode evaluation \
    --max_questions 100
```

**Output**: `results/evaluation_qwen_qwen2-vl-7b-instruct_TIMESTAMP.csv`

Note the accuracy for comparison.

### Step 2: Fine-tune the Model

Follow the training guide in `training/README.md` to fine-tune the model.

### Step 3: Evaluate Fine-tuned Model

```bash
# Generate responses with fine-tuned model
PYTHONPATH=. python evaluation/evaluate_qwen2vl.py \
    --model outputs/circuitsense_finetune/final_model \
    --dataset_path datasets/symbolic_level15_27 \
    --mode inference \
    --max_questions 100

# Evaluate fine-tuned model responses
PYTHONPATH=. python evaluation/evaluate_qwen2vl.py \
    --model outputs/circuitsense_finetune/final_model \
    --dataset_path datasets/symbolic_level15_27 \
    --mode evaluation \
    --max_questions 100
```

**Output**: `results/evaluation_final_model_TIMESTAMP.csv`

### Step 4: Compare Results

Compare the accuracy metrics from both CSV files to see if fine-tuning improved performance.

## Output Files

### CSV Results

Each run generates a CSV file with:

- `question_number`: Question identifier (e.g., `q1`, `q2`)
- `question_text`: Full question text
- `model_answer`: Extracted equation from model response
- `correct_answer`: Ground truth equation
- `is_correct`: Boolean indicating if equation matches (True/False/None)
- `comparison_details`: Details about the comparison (e.g., "Equal using Direct equality")
- `thinking_process`: Model's reasoning (if extracted)
- `processing_time`: Time taken to process the question

**Accuracy Calculation**:
- `is_correct = True`: Equation matches ground truth
- `is_correct = False`: Equation does not match
- `is_correct = None`: Comparison timed out or failed (excluded from accuracy)

### Saved Responses

Each question folder contains the model's full response:

```
datasets/symbolic_level15_27/q1/
├── q1_question.txt
├── q1_image.png
├── q1_ta.txt
└── q1_<model_tag>.txt  # Model response (created during inference)
```

The model tag is automatically generated from the model path.

### Intermediate Results

Intermediate results are saved every 10 questions:
- `results/temp_inference_<model_tag>_10.csv`
- `results/temp_inference_<model_tag>_20.csv`
- etc.

Useful for monitoring progress on long runs.

## Equation Comparison

The script uses the same robust equation comparison logic as `benchmark_symbolic_equations.py`:

1. **Symbolic Comparison**: Uses SymPy to compare equations symbolically
   - Direct equality checks
   - Difference simplification
   - Expansion and factorization
   - Sign-invariant checks (accounts for equations that differ only by sign)

2. **Numerical Fallback**: If symbolic comparison fails or times out:
   - Evaluates both equations at multiple random points
   - Checks if results match within tolerance
   - Handles sign-invariance

3. **Timeout Protection**: 
   - Per-strategy timeout: 5 seconds
   - Global comparison timeout: 120 seconds
   - Questions that timeout are marked as `is_correct = None` and excluded from accuracy

## Troubleshooting

### Out of Memory Errors

If you encounter OOM errors:

1. **Reduce batch size**: The script processes one question at a time, but you can:
   - Use quantization: Load model with 8-bit or 4-bit quantization
   - Use CPU: Set `--device cpu` (much slower)
   - Reduce max tokens: Set `--max_new_tokens 256`

2. **Use smaller dtype**: Try `--torch_dtype float16` instead of `bfloat16`

### Model Loading Issues

**LoRA Adapter Detection**:
- Ensure `adapter_config.json` exists in the model directory
- The script automatically detects LoRA models and loads the base model + adapter

**HuggingFace Model IDs**:
- Ensure you have access to the model (if gated)
- Set `HF_TOKEN` environment variable if needed

### Slow Evaluation

- Use GPU: Ensure `CUDA_VISIBLE_DEVICES` is set correctly
- Use flash attention: Don't use `--no_flash_attention` flag
- Process in batches: Use `inference` mode first, then `evaluation` mode separately

### Missing Responses

If evaluation mode can't find responses:
- Check that inference was run first
- Verify the model tag matches (responses are saved with model-specific tags)
- Check file names: `q1_<model_tag>.txt`

## Advanced Usage

### Custom Prompt Format

The prompt is defined in the `create_prompt()` method. You can modify it to experiment with different prompt formats.

### Evaluating Multiple Models

To compare multiple models:

```bash
# Model 1
PYTHONPATH=. python evaluation/evaluate_qwen2vl.py \
    --model Qwen/Qwen2-VL-7B-Instruct \
    --dataset_path datasets/symbolic_level15_27 \
    --mode full --max_questions 50

# Model 2
PYTHONPATH=. python evaluation/evaluate_qwen2vl.py \
    --model outputs/model2/final_model \
    --dataset_path datasets/symbolic_level15_27 \
    --mode full --max_questions 50
```

Each model's responses are saved with different tags, so they won't overwrite each other.

### Resume from Interruption

If a run is interrupted:
- Responses already saved are preserved
- Re-run with `--mode evaluation` to evaluate existing responses
- Re-run with `--mode inference` to continue generating missing responses

The script skips questions that already have saved responses in inference mode.

## Example Output

```
2025-12-03 12:00:00 - INFO - Loading model from: Qwen/Qwen2-VL-7B-Instruct
2025-12-03 12:00:30 - INFO - Model loaded successfully!
2025-12-03 12:00:30 - INFO - Running inference on 10 questions: ['q1', 'q2', ...]

======================================================================
INFERENCE q1 (1/10)
Elapsed time: 31.2 seconds
======================================================================
2025-12-03 12:00:31 - INFO - Question: Derive the nodal equation for node 3...
2025-12-03 12:00:45 - INFO - ✓ Generated and saved response for q1

...

======================================================================
EVALUATION RESULTS SUMMARY
======================================================================
Total Questions: 10
Evaluated (excl. skipped): 10
Correct Answers: 7
Accuracy: 70.00%
Total Time: 245.3 seconds
Average Time per Question: 24.5 seconds
```

## Performance Tips

1. **Use GPU**: Significantly faster than CPU
2. **Use Flash Attention**: Enabled by default, don't disable unless necessary
3. **Batch Processing**: Use `inference` mode separately to generate all responses, then evaluate
4. **Intermediate Saves**: Results are saved every 10 questions automatically

## Notes

- The script reuses equation comparison logic from `benchmark_symbolic_equations.py`
- Responses are saved with model-specific tags to avoid conflicts
- Comparison timeouts don't crash the script; questions are skipped
- All results are saved as CSV for easy analysis

